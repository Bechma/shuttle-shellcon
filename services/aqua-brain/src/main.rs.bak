use shuttle_axum::axum::extract::{Path, Query, State};
use shuttle_axum::axum::http::StatusCode;
use shuttle_axum::axum::response::IntoResponse;
use shuttle_axum::axum::routing::get;
use shuttle_axum::axum::Json;
use shuttle_axum::axum::Router;
// CORS removed - managed by frontend
use serde::{Deserialize, Serialize};
// No unused imports
use tracing;
use thiserror::Error;

// Define a custom error type for better error handling
#[derive(Debug, Error)]
pub enum ApiError {
    #[error("System status unavailable: {0}")]
    SystemStatusUnavailable(String),
    
    #[error("Analysis failed: {0}")]
    AnalysisFailed(String),
    
    #[error("Species data unavailable: {0}")]
    SpeciesDataUnavailable(String),
    
    #[error("External service error: {0}")]
    ExternalService(#[from] reqwest::Error),
    
    #[error("Internal server error: {0}")]
    InternalError(String),
}

// Implement IntoResponse for our custom error type
impl IntoResponse for ApiError {
    fn into_response(self) -> shuttle_axum::axum::response::Response {
        let (status, error_message) = match &self {
            ApiError::SystemStatusUnavailable(_) => (StatusCode::SERVICE_UNAVAILABLE, self.to_string()),
            ApiError::AnalysisFailed(_) => (StatusCode::INTERNAL_SERVER_ERROR, self.to_string()),
            ApiError::SpeciesDataUnavailable(_) => (StatusCode::SERVICE_UNAVAILABLE, self.to_string()),
            ApiError::ExternalService(_) => (StatusCode::BAD_GATEWAY, "External service error".to_string()),
            ApiError::InternalError(_) => (StatusCode::INTERNAL_SERVER_ERROR, self.to_string()),
        };
        
        // Log the error with structured fields
        tracing::error!(
            error.type = std::any::type_name::<Self>(),
            error.message = %error_message,
            error.status = %status.as_u16(),
            "API error occurred"
        );
        
        // Return status code and JSON error message
        (status, Json(serde_json::json!({
            "error": error_message,
            "status": status.as_u16(),
            "timestamp": chrono::Utc::now().to_rfc3339()
        }))).into_response()
    }
}

#[derive(Clone)]
struct AppState {}

#[derive(Serialize, Deserialize, Clone)]
struct AnalysisResult {
    tank_id: String,
    species_id: i32,
    timestamp: String,
    temperature_status: &'static str,
    ph_status: &'static str,
    oxygen_status: &'static str,
    feeding_status: &'static str,
    overall_health: &'static str,
    recommendations: Vec<&'static str>,
}

#[derive(Deserialize, Clone)]
struct AnalysisParams {
    tank_id: Option<String>,
    species_id: Option<i32>,
}

#[shuttle_runtime::main]
async fn axum() -> shuttle_axum::ShuttleAxum {
    // Initialize state - no clients needed following our KISS architecture
    let state = AppState {};
    
    // Build router
    let router = Router::new()
        .route("/api/analysis/tanks", get(get_all_tank_analysis))
        .route("/api/analysis/tanks/:tank_id", get(get_tank_analysis_by_id))
        .route("/api/challenges/current", get(get_current_challenge))
        .route("/api/challenges/test/1", get(test_challenge_1))
        .route("/api/challenges/3/validate", get(validate_memory_optimization))
        // Challenge solution validation should be in the service where the implementation resides
        // For Challenge #1, validation is done in the aqua-monitor service
        .route("/api/health", get(health_check))
        .with_state(state);
    
    Ok(router.into())
}


#[derive(Serialize)]
struct ChallengeSolution {
    code: String,
    explanation: String,
    lecture: String,
}

async fn get_current_challenge() -> impl IntoResponse {
    // Create a span for tracking challenge metadata requests
    let span = tracing::info_span!("challenge_metadata_request");
    let _guard = span.enter();
    let request_id = uuid::Uuid::new_v4().to_string();
    
    tracing::info!(
        request_id = %request_id,
        operation = "get_challenge_metadata",
        "Providing challenge metadata"
    );
    
    // Define detailed challenge information
    let challenge_1_solution = ChallengeSolution {
        code: r#"// Before: Blocking I/O
// let config = std::fs::read_to_string("./config/tank_settings.json")
//     .unwrap_or_else(|_| "{}".to_string());
// Also remove the artificial delay
// std::thread::sleep(std::time::Duration::from_millis(100));

// After: Async I/O
let config = tokio::fs::read_to_string("./config/tank_settings.json").await
    .unwrap_or_else(|_| "{}".to_string());
"#.to_string(),
        explanation: "This solution replaces a blocking file I/O operation with an async version that won't block the entire thread, significantly improving response times and system throughput.".to_string(),
        lecture: r#"# Understanding Blocking vs. Non-Blocking I/O in Rust

## The Problem with Blocking I/O

In an async Rust application, using synchronous I/O operations like 'std::fs::read_to_string()' blocks the entire thread until the operation completes. This means:

- No other tasks can run on that thread while waiting for I/O
- Overall throughput is reduced
- Response times become inconsistent
- System scalability is limited

## The Async I/O Solution

Replacing 'std::fs' with 'tokio::fs' makes the I/O operation truly asynchronous:

'''
// Before: Blocking I/O
// let config = std::fs::read_to_string("./config/tank_settings.json")
//     .unwrap_or_else(|_| "{}".to_string());
// Also remove the artificial delay
// std::thread::sleep(std::time::Duration::from_millis(100));

// After: Async I/O
let config = tokio::fs::read_to_string("./config/tank_settings.json").await
    .unwrap_or_else(|_| "{}".to_string());
"""

## Performance Benefits

Async I/O provides several key benefits:

1. **Concurrency**: Multiple I/O operations can be in progress simultaneously
2. **Throughput**: The system can handle more requests per second
3. **Responsiveness**: Critical operations do not get blocked by slow I/O
4. **Resource Efficiency**: Thread resources aren't wasted waiting for I/O

## How Tokio's Async I/O Works

Under the hood, Tokio uses an event-driven architecture with an I/O event queue:

1. When you call an async function and '.await' it, the current task is suspended
2. The I/O operation is registered with the OS's async I/O facilities
3. The thread is free to process other tasks while waiting
4. When the OS signals completion, Tokio wakes up the task
5. The task continues from where it left off

This cooperative multitasking model is the foundation of modern high-performance services.

## Best Practices

- Use async I/O for all potentially slow operations (file, network, etc.)
- Keep CPU-intensive work off the async runtime when possible
- Remember that '.await' points are where your function can be suspended
- Use proper error handling with async operations
"#.to_string(),
    };
    
    let challenge_2_solution = ChallengeSolution {
        code: r#"// Before: Inefficient LIKE queries with case-sensitivity
// For name search:
// sqlx::query("SELECT * FROM species WHERE name LIKE $1")
//     .bind(format!("%{}%", name))
// 
// For scientific_name search:
// sqlx::query("SELECT * FROM species WHERE scientific_name LIKE $1")
//     .bind(format!("%{}%", scientific_name))

// After: Optimized ILIKE queries with trigram index support
// For name search:
sqlx::query("SELECT * FROM species WHERE name ILIKE $1")
    .bind(format!("%{}%", name))

// For scientific_name search:
sqlx::query("SELECT * FROM species WHERE scientific_name ILIKE $1")
    .bind(format!("%{}%", scientific_name))
"#.to_string(),
        explanation: "This solution replaces case-sensitive LIKE queries with case-insensitive ILIKE queries that can utilize PostgreSQL's trigram indexes, dramatically improving search performance.".to_string(),
        lecture: r#"# Database Query Optimization with Indexes

## The Problem with Non-Indexed LIKE Queries

When searching text fields with LIKE operators in SQL databases, performance suffers dramatically without proper indexing:

- Each search requires a full table scan, examining every row
- Query time grows linearly with table size
- Case-sensitive searches miss potential matches
- System throughput decreases under load

## The Optimized Solution

Two key improvements make our queries fast and user-friendly:

"""sql
-- Before: Inefficient LIKE query with case-sensitivity
-- SELECT * FROM species WHERE name LIKE '%search_term%'

-- After: Optimized ILIKE query (works with trigram index)
SELECT * FROM species WHERE name ILIKE '%search_term%'
"""

## How PostgreSQL Trigram Indexes Work

Trigram indexes break text into 3-character sequences, enabling efficient pattern matching:

1. The text "Clownfish" produces trigrams: "clo", "low", "own", "wnf", "nfi", "fis", "ish"
2. Searches for similar patterns can use the index rather than scanning the table
3. ILIKE queries automatically benefit from trigram indexes when available

## Performance Benefits

- **Search Speed**: Queries can be 10-100x faster on large tables
- **Case Insensitivity**: Finding "clownfish" when users search for "Clownfish"
- **Scalability**: Performance stays consistent as the database grows
- **User Experience**: Faster, more intuitive search results

## Best Practices

- Use 'CREATE INDEX idx_name ON table USING gin (column gin_trgm_ops)' for trigram indexes
- Consider partial indexes if you only search a subset of data
- Analyze query plans with EXPLAIN ANALYZE to verify index usage
- Monitor index size and rebuild periodically for optimal performance
"#.to_string()
    };
    
    let challenge_3_solution = ChallengeSolution {
        code: r#"// Before: Creating new String objects for every field
// AnalysisResult {
//     tank_id: tank_id,
//     species_id: params.species_id.unwrap_or(1),
//     timestamp: chrono::Utc::now().to_rfc3339(),
//     temperature_status: "warning".to_string(),
//     ph_status: "critical".to_string(),
//     oxygen_status: "normal".to_string(),
//     feeding_status: "overdue".to_string(),
//     overall_health: "at_risk".to_string(),
//     recommendations: vec![
//         "Reduce temperature by 2°C".to_string(),
//         "Adjust pH to 7.2-7.5 range".to_string(),
//         "Administer emergency feeding".to_string(),
//     ],
// }

// After: Using static string references
// First, define static string constants
static WARNING: &str = "warning";
static CRITICAL: &str = "critical";
static NORMAL: &str = "normal";
static OVERDUE: &str = "overdue";
static AT_RISK: &str = "at_risk";

// Then use them in the AnalysisResult
AnalysisResult {
    tank_id: tank_id,
    species_id: params.species_id.unwrap_or(1),
    timestamp: chrono::Utc::now().to_rfc3339(),
    temperature_status: WARNING,
    ph_status: CRITICAL,
    oxygen_status: NORMAL,
    feeding_status: OVERDUE,
    overall_health: AT_RISK,
    recommendations: vec![
        "Reduce temperature by 2°C",
        "Adjust pH to 7.2-7.5 range",
        "Administer emergency feeding",
    ],
}
"#.to_string(),
        explanation: "This solution optimizes memory usage by using static string references (&str) instead of creating new String objects for every analysis result. This reduces memory allocations and improves performance.".to_string(),
        lecture: r#"# Memory Optimization in Rust: Static Strings vs. Dynamic Allocations

## The Problem with Excessive String Allocations

In Rust, when you see '.to_string()' or 'String::from()', your program is doing extra work: it's reserving memory, copying characters, and tracking when to clean up. Think of it like making a photocopy of a document instead of just pointing to the original.

This extra work causes several problems:

- 📈 **More Memory Used**: Each 'String' takes up space on the heap (a region of memory for dynamic data)
- 🐢 **Slower Performance**: Creating and cleaning up memory takes CPU time
- 🔄 **Memory Fragmentation**: Like having scattered free seats in a theater instead of consecutive ones
- 🔄 **More Garbage Collection**: The system has to work harder to clean up unused strings
- 🔋 **Higher Energy Usage**: More CPU cycles means more power consumption

## The Static String Reference Solution

Instead of making copies, you can use references ('&str') to point to existing strings, especially for text that doesn't change:

'''
// Before: Making a new copy of "normal" in memory
let status = "normal".to_string();

// After: Just pointing to the existing "normal" text (no extra memory needed)
let status = "normal";  // This is actually &str type behind the scenes
"""

### Visual Example: Menu in a Restaurant

Imagine two approaches for a restaurant menu:

- **String approach**: Print a full copy of the menu for each customer (uses more paper, takes time to print)
- **&str approach**: Have one menu on the wall that customers just look at (saves resources)

## Performance Benefits Explained

Using '&str' instead of 'String' when possible gives you:

1. **Zero Allocation**: No extra memory needed = less RAM used
2. **Speed**: Faster program execution without memory management overhead
3. **Efficiency**: Better use of CPU cache (like having ingredients closer at hand when cooking)

## Beginner-Friendly Best Practices

### For Function Parameters:

'''
// Good: Accept &str when you just need to read
fn greet(name: &str) { println!("Hello, {}", name); }

// Less efficient: Forces callers to create String copies
fn greet(name: String) { println!("Hello, {}", name); }
"""

### For Constant Values:

'''
// Define once, use many times without copies
static ERROR_MESSAGE: &str = "Something went wrong";
"""

### In Structs:

'''
// Good for fixed values that won't change:
struct Dog {
    breed: &'static str,  // Like "Labrador" that won't change
    name: String,         // Like "Buddy" that came from user input
}
"""

## When To Use Each Type (Simple Guide)

Use 'String' when:
- The text comes from user input or files
- You need to build or change the text
- You're storing text in a collection for a long time

Use '&str' when:
- You just need to read the text
- The text is a literal in your code like "hello"
- You're passing text briefly to a function

## Real-World Example:

'''
// A user profile system that optimizes memory

// Constants (zero memory allocation)
static DEFAULT_LOCATION: &str = "Unknown";
static DEFAULT_BIO: &str = "This user hasn't added a bio yet";

struct User {
    // User-provided data needs String
    username: String,
    email: String,
    
    // Fixed options can use &'static str
    account_type: &'static str,
    
    // Optional fields use Option<String> if user-provided
    location: Option<String>,
    
    // Mutable with default value
    bio: String,
}

impl User {
    fn new(username: String, email: String) -> Self {
        User {
            username,
            email,
            account_type: "standard",  // No allocation here!
            location: None,
            bio: DEFAULT_BIO.to_string(),  // Only one allocation here
        }
    }
    
    // Function accepts &str to be flexible
    fn set_location(&mut self, location: &str) {
        self.location = Some(location.to_string());
    }
}
"""

By understanding when to use 'String' vs '&str', you'll write more efficient Rust code that uses less memory and runs faster!

## Resource Management in Rust: Avoiding Leaks with Static Clients

## The Problem with Creating HTTP Clients Per Request

In HTTP-heavy applications, creating a new client for every request is a common but serious performance issue:

- **Excessive Memory Usage**: Each client allocates its own connection pool and internal buffers
- **Slow Performance**: Client initialization takes time (TLS setup, DNS resolution caching)
- **Connection Churn**: Constant creation/destruction of TCP connections
- **Resource Exhaustion**: Can lead to file descriptor limits being reached
- **Poor Connection Reuse**: HTTP keep-alive benefits are lost

## The Static Client Solution

By creating a single, reusable HTTP client, we solve these problems:

"""rust
use lazy_static::lazy_static;

// Create a single shared client for the entire application
lazy_static! {
    static ref HTTP_CLIENT: reqwest::Client = reqwest::Client::builder()
        .timeout(Duration::from_secs(30))
        .build()
        .expect("Failed to create HTTP client");
}

async fn make_request() {
    // Use the shared client
    let response = HTTP_CLIENT.get("https://api.example.com").send().await?;
    // Process response...
}
"""

## Key Implementation Options

### 1. Using lazy_static

The 'lazy_static' crate initializes static values on first use:

"""rust
use lazy_static::lazy_static;

lazy_static! {
    static ref HTTP_CLIENT: reqwest::Client = reqwest::Client::new();
}
"""

### 2. Using once_cell (More Modern)

The 'once_cell' crate is now the preferred approach:

"""rust
use once_cell::sync::Lazy;

static HTTP_CLIENT: Lazy<reqwest::Client> = Lazy::new(|| reqwest::Client::new());
"""

## Performance Benefits

- **Connection Pooling**: HTTP connections are reused
- **DNS Caching**: Hostnames are resolved once and cached
- **TLS Session Resumption**: TLS handshakes can be reused
- **Memory Efficiency**: Dramatic reduction in allocation overhead
- **Scalability**: Applications can handle more concurrent requests

## Common Pitfalls to Avoid

- **Client Configuration**: Set appropriate timeouts and pool limits
- **Thread Safety**: Ensure your static client is thread-safe (reqwest Client already is)
- **Error Handling**: Handle client initialization failures gracefully
- **Testing**: Static clients can make testing harder; consider dependency injection

## Monitoring Resource Usage

It is important to monitor the effects of your optimization:

- Track memory usage before and after implementation
- Monitor open file descriptors
- Observe connection pooling behavior
- Measure request latency improvements

Properly implementing a shared static HTTP client is a small change with a huge impact on your service reliability and performance.
- 🧠 **Excessive Memory Usage**: Each client allocates its own connection pool and internal buffers
- 🐢 **Slow Performance**: Client initialization takes time (TLS setup, DNS resolution caching)
- 🔌 **Connection Churn**: Constant creation/destruction of TCP connections
- ⏱️ **Resource Exhaustion**: Can lead to file descriptor limits being reached
- 🚫 **Poor Connection Reuse**: HTTP keep-alive benefits are lost

## The Static Client Solution

By creating a single, reusable HTTP client, we solve these problems:

'''
use lazy_static::lazy_static;

// Create a single shared client for the entire application
lazy_static! {
    static ref HTTP_CLIENT: reqwest::Client = reqwest::Client::builder()
        .timeout(Duration::from_secs(30))
        .build()
        .expect("Failed to create HTTP client");
}

async fn make_request() {
    // Use the shared client
    let response = HTTP_CLIENT.get("https://api.example.com").send().await?;
    // Process response...
}
"""

## Key Implementation Options

### 1. Using lazy_static

The 'lazy_static' crate initializes static values on first use:

'''
use lazy_static::lazy_static;

lazy_static! {
    static ref HTTP_CLIENT: reqwest::Client = reqwest::Client::new();
}
"""

### 2. Using once_cell (More Modern)

The 'once_cell' crate is now the preferred approach:

'''
use once_cell::sync::Lazy;

static HTTP_CLIENT: Lazy<reqwest::Client> = Lazy::new(|| reqwest::Client::new());
"""

## Performance Benefits

- **Connection Pooling**: HTTP connections are reused
- **DNS Caching**: Hostnames are resolved once and cached
- **TLS Session Resumption**: TLS handshakes can be reused
- **Memory Efficiency**: Dramatic reduction in allocation overhead
- **Scalability**: Applications can handle more concurrent requests

## Common Pitfalls to Avoid

- **Client Configuration**: Set appropriate timeouts and pool limits
- **Thread Safety**: Ensure your static client is thread-safe (reqwest Client already is)
- **Error Handling**: Handle client initialization failures gracefully
- **Testing**: Static clients can make testing harder; consider dependency injection

## Monitoring Resource Usage

It is important to monitor the effects of your optimization:

- Track memory usage before and after implementation
- Monitor open file descriptors
- Observe connection pooling behavior
- Measure request latency improvements

Properly implementing a shared static HTTP client is a small change with a huge impact on your service reliability and performance.
"#.to_string(),
    };

## The Problem with Excessive String Allocations

In Rust, when you see '.to_string()' or 'String::from()', your program is doing extra work: it's reserving memory, copying characters, and tracking when to clean up. Think of it like making a photocopy of a document instead of just pointing to the original.

This extra work causes several problems:

- 📈 **More Memory Used**: Each 'String' takes up space on the heap (a region of memory for dynamic data)
- 🐢 **Slower Performance**: Creating and cleaning up memory takes CPU time
- 🔄 **Memory Fragmentation**: Like having scattered free seats in a theater instead of consecutive ones
- 🔄 **More Garbage Collection**: The system has to work harder to clean up unused strings
- 🔋 **Higher Energy Usage**: More CPU cycles means more power consumption

## The Static String Reference Solution

Instead of making copies, you can use references ('&str') to point to existing strings, especially for text that doesn't change:

'''
// Before: Making a new copy of "normal" in memory
let status = "normal".to_string();

// After: Just pointing to the existing "normal" text (no extra memory needed)
let status = "normal";  // This is actually &str type behind the scenes
"""

### Visual Example: Menu in a Restaurant

Imagine two approaches for a restaurant menu:

- **String approach**: Print a full copy of the menu for each customer (uses more paper, takes time to print)
- **&str approach**: Have one menu on the wall that customers just look at (saves resources)

## Performance Benefits Explained

Using '&str' instead of 'String' when possible gives you:

1. **Zero Allocation**: No extra memory needed = less RAM used
2. **Speed**: Faster program execution without memory management overhead
3. **Efficiency**: Better use of CPU cache (like having ingredients closer at hand when cooking)

## Beginner-Friendly Best Practices

### For Function Parameters:

'''
// Good: Accept &str when you just need to read
fn greet(name: &str) { println!("Hello, {}", name); }

// Less efficient: Forces callers to create String copies
fn greet(name: String) { println!("Hello, {}", name); }
"""

### For Constant Values:

'''
// Define once, use many times without copies
static ERROR_MESSAGE: &str = "Something went wrong";
"""

### In Structs:

'''
// Good for fixed values that won't change:
struct Dog {
    breed: &'static str,  // Like "Labrador" that won't change
    name: String,         // Like "Buddy" that came from user input
}
"""

## When To Use Each Type (Simple Guide)

Use 'String' when:
- The text comes from user input or files
- You need to build or change the text
- You're storing text in a collection for a long time

Use '&str' when:
- You just need to read the text
- The text is a literal in your code like "hello"
- You're passing text briefly to a function

## Real-World Example:

'''
// A user profile system that optimizes memory

// Constants (zero memory allocation)
static DEFAULT_LOCATION: &str = "Unknown";
static DEFAULT_BIO: &str = "This user hasn't added a bio yet";

struct User {
    // User-provided data needs String
    username: String,
    email: String,
    
    // Fixed options can use &'static str
    account_type: &'static str,
    
    // Optional fields use Option<String> if user-provided
    location: Option<String>,
    
    // Mutable with default value
    bio: String,
}

impl User {
    fn new(username: String, email: String) -> Self {
        User {
            username,
            email,
            account_type: "standard",  // No allocation here!
            location: None,
            bio: DEFAULT_BIO.to_string(),  // Only one allocation here
        }
    }
    
    // Function accepts &str to be flexible
    fn set_location(&mut self, location: &str) {
        self.location = Some(location.to_string());
    }
}
"""

By understanding when to use 'String' vs '&str', you'll write more efficient Rust code that uses less memory and runs faster!
"#.to_string(),
    };
    
    // Define Challenge #4 solution details
    let challenge_4_solution = ChallengeSolution {
        code: r#"// Before: Creating a new client for every request
// This causes resource leaks and excessive memory usage
async fn get_sensor_status(State(_state): State<AppState>) -> impl IntoResponse {
    // Create a span for sensor status check
    let span = tracing::info_span!("tank_sensor_status_check");
    let _guard = span.enter();
    let _start = std::time::Instant::now();

    // ❌ BAD: This intentionally creates a new client for every request
    // causing resource leakage
    let client = reqwest::Client::new();
    
    // Rest of the function...
}

// After: Using a shared static client
// Import the necessary crates
use lazy_static::lazy_static;
// or
// use once_cell::sync::Lazy;

// Define a static HTTP client that can be reused across requests
lazy_static! {
    static ref HTTP_CLIENT: reqwest::Client = reqwest::Client::new();
}
// or using once_cell
// static HTTP_CLIENT: Lazy<reqwest::Client> = Lazy::new(|| reqwest::Client::new());

async fn get_sensor_status(State(_state): State<AppState>) -> impl IntoResponse {
    // Create a span for sensor status check
    let span = tracing::info_span!("tank_sensor_status_check");
    let _guard = span.enter();
    let _start = std::time::Instant::now();

    // ✅ GOOD: Use the shared static client
    // No new client is created here
    let client = &HTTP_CLIENT;
    
    // Rest of the function remains the same
    // but now uses the shared client
}
"#.to_string(),
        explanation: "This solution addresses the resource leak by creating a static HTTP client using lazy_static or once_cell instead of creating a new client for every request. The static client is initialized only once and reused across all requests, significantly reducing memory usage and resource consumption.".to_string(),
        lecture: r#"# Resource Management in Rust: Avoiding Leaks with Static Clients

## The Problem with Creating HTTP Clients Per Request

In HTTP-heavy applications, creating a new client for every request is a common but serious performance issue:

- 🧠 **Excessive Memory Usage**: Each client allocates its own connection pool and internal buffers
- 🐢 **Slow Performance**: Client initialization takes time (TLS setup, DNS resolution caching)
- 🔌 **Connection Churn**: Constant creation/destruction of TCP connections
- ⏱️ **Resource Exhaustion**: Can lead to file descriptor limits being reached
- 🚫 **Poor Connection Reuse**: HTTP keep-alive benefits are lost

## The Static Client Solution

By creating a single, reusable HTTP client, we solve these problems:

'''
use lazy_static::lazy_static;

// Create a single shared client for the entire application
lazy_static! {
    static ref HTTP_CLIENT: reqwest::Client = reqwest::Client::builder()
        .timeout(Duration::from_secs(30))
        .build()
        .expect("Failed to create HTTP client");
}

async fn make_request() {
    // Use the shared client
    let response = HTTP_CLIENT.get("https://api.example.com").send().await?;
    // Process response...
}
"""

## Key Implementation Options

### 1. Using lazy_static

The 'lazy_static' crate initializes static values on first use:

'''
use lazy_static::lazy_static;

lazy_static! {
    static ref HTTP_CLIENT: reqwest::Client = reqwest::Client::new();
}
"""

### 2. Using once_cell (More Modern)

The 'once_cell' crate is now the preferred approach:

'''
use once_cell::sync::Lazy;

static HTTP_CLIENT: Lazy<reqwest::Client> = Lazy::new(|| reqwest::Client::new());
"""

## Performance Benefits

- **Connection Pooling**: HTTP connections are reused
- **DNS Caching**: Hostnames are resolved once and cached
- **TLS Session Resumption**: TLS handshakes can be reused
- **Memory Efficiency**: Dramatic reduction in allocation overhead
- **Scalability**: Applications can handle more concurrent requests

## Common Pitfalls to Avoid

- **Client Configuration**: Set appropriate timeouts and pool limits
- **Thread Safety**: Ensure your static client is thread-safe (reqwest Client already is)
- **Error Handling**: Handle client initialization failures gracefully
- **Testing**: Static clients can make testing harder; consider dependency injection

## Monitoring Resource Usage

It is important to monitor the effects of your optimization:

- Track memory usage before and after implementation
- Monitor open file descriptors
- Observe connection pooling behavior
- Measure request latency improvements

Properly implementing a shared static HTTP client is a small change with a huge impact on your service reliability and performance.
"#.to_string(),
    };
    
    // Return challenge metadata as JSON with detailed information including validation endpoints
    Json(serde_json::json!({
        "challenges": [
            {
                "id": 1,
                "name": "latency-issue ",
                "title": "The Sluggish Sensor ",
                "description": "The environmental monitoring system is experiencing severe delays, preventing timely readings of tank conditions. The file I/O operations seem particularly slow, with response times far exceeding what should be expected even for standard disk operations. Every second counts when maintaining delicate ecosystems! ",
                "hint": "Look for blocking operations in the get_tank_readings function. In async Rust, using std::fs::read_to_string blocks the entire thread. Also, check if there are any artificial delays that might be contributing to the slowdown. The solution involves both using the right async I/O method and ensuring there are no unnecessary waits.",
                "service": "aqua-monitor ",
                "file": "src/main.rs ",
                "function": "get_tank_readings ",
                "status": "degraded",
                "validation_endpoint": {
                    "service": "aqua-monitor ",
                    "url": "/api/challenges/1/validate ",
                    "method": "GET"
                },
                "solution": challenge_1_solution
            },
            {
                "id": 2,
                "name": "query-optimization ",
                "title": "The Query Conundrum ",
                "description": "The species database is responding slowly to searches, making it difficult to quickly identify species information. The current implementation uses case-sensitive queries that do not utilize database indexes effectively. ",
                "hint": "Look at the get_species function in the species-hub service. The SQL queries are using LIKE for case-sensitive searches, which can be slow. Consider using a more efficient query approach that maintains case-insensitivity.",
                "service": "species-hub ",
                "file": "src/main.rs ",
                "function": "get_species ",
                "status": "degraded",
                "validation_endpoint": {
                    "service": "species-hub ",
                    "url": "/api/challenges/2/validate ",
                    "method": "GET"
                },
                "solution": challenge_2_solution
            },
            {
                "id": 3,
                "name": "memory-optimization ",
                "title": "The Memory Miser ",
                "description": "The analysis engine is consuming excessive memory due to inefficient string handling. Each analysis result creates multiple new String objects, leading to high memory usage and frequent garbage collection. ",
                "hint": "Look at the get_analysis_result function in the aqua-brain service. The current implementation creates new String objects for every field in the analysis results. Consider using static string references (&str) instead of String objects to reduce memory allocations.",
                "service": "aqua-brain ",
                "file": "src/main.rs ",
                "function": "get_analysis_result ",
                "status": "degraded",
                "validation_endpoint": {
                    "service": "aqua-brain ",
                    "url": "/api/challenges/3/validate ",
                    "method": "GET"
                },
                "solution": challenge_3_solution
            },
            {
                "id": 4,
                "name": "resource-leak ",
                "title": "The Leaky Connection ",
                "description": "The sensor status API is creating a new HTTP client for every request, causing excessive resource usage and potential memory leaks. ",
                "hint": "Create a shared, static client that can be reused across requests instead of creating a new one each time.",
                "service": "aqua-monitor ",
                "file": "src/main.rs ",
                "function": "get_sensor_status ",
                "status": "degraded",
                "validation_endpoint": {
                    "service": "aqua-monitor ",
                    "url": "/api/challenges/4/validate ",
                    "method": "GET"
                },
                "solution": challenge_4_solution
            },
            {
                "id": 5,
                "name": "shared-state-mutex ",
                "title": "Safe Shared State ",
                "description": "You need to maintain a shared counter or cache across requests, but naive approaches can cause data races or panics. Use Rust async-safe Mutex to implement correct shared state. ",
                "hint": "Use Arc<Mutex<T>> or tokio::sync::Mutex for shared mutable state. Beware of holding locks across .await points!",
                "service": "aqua-brain ",
                "file": "src/main.rs ",
                "function": "shared_state_example ",
                "status": "degraded",
                "validation_endpoint": {
                    "service": "aqua-brain ",
                    "url": "/api/challenges/5/validate-solution ",
                    "method": "GET"
                },
                "solution": ChallengeSolution {
                    code: "// Example using Arc<tokio::sync::Mutex<T>>".to_string(),
                    explanation: "This solution demonstrates how to safely share and mutate state across async requests using a lock.".to_string(),
                    lecture: "Lecture on Mutex, Arc, and shared state in async Rust".to_string()
                }
            }
        ],
        "total": 5,
        "solved": 0,
    }))
}

// Dedicated endpoint for testing Challenge #1
async fn test_challenge_1() -> impl IntoResponse {
    // Create a span for tracking sensor latency diagnostics
    let span = tracing::info_span!("sensor_latency_diagnostic");
    let _guard = span.enter();
    
    tracing::info!("Sensor response time diagnostic requested");
    
    let response = serde_json::json!({
        "id": 1,
        "name": "The Sluggish Sensor ",
        "message": "For validation, please call the aqua-monitor service at /api/challenges/1/validate",
        "hint": "Replace std::fs::read_to_string with tokio::fs::read_to_string and add .await to make the file I/O operation async.",
        "system_component": {
            "name": "Analysis Engine ",
            "status": "normal",
            "description": "Analysis engine operating normally "
        }
    });
    
    tracing::info!(
        challenge_id = 1,
        challenge_name = "latency-issue",
        "Challenge #1 test endpoint called - redirecting to service-specific validation"
    );
    
    Json(response)
}

async fn health_check() -> impl IntoResponse {
    StatusCode::OK
}



// Define a summary struct for collection response
#[derive(Serialize)]
struct TankSummary {
    tank_id: String,
    species_id: i32,
    species_name: String,
    overall_health: &'static str,
    timestamp: String,
}

// Map species_id to species_name for the demo
fn get_species_name(species_id: i32) -> String {
    match species_id {
        1 => "Neon Tetra".to_string(),
        2 => "Clownfish".to_string(),
        3 => "Blue Tang".to_string(),
        4 => "Guppy".to_string(),
        5 => "Betta".to_string(),
        _ => format!("Unknown Species (ID: {})", species_id),
    }
}

// Handler for all tanks analysis - returns summarized information
async fn get_all_tank_analysis(
    Query(params): Query<AnalysisParams>,
    State(_state): State<AppState>,
) -> impl IntoResponse {
    // Create a span for tracking multi-tank environmental analysis
    let span = tracing::info_span!("multi_tank_analysis");
    let _guard = span.enter();
    
    // Add request ID for correlation and timing
    let request_id = uuid::Uuid::new_v4().to_string();
    let start_time = std::time::Instant::now();
    
    tracing::info!(
        request_id = %request_id,
        operation = "multi_tank_analysis",
        tanks_requested = "all",
        "Starting multi-tank environmental analysis"
    );
    // Defined tank IDs in our system
    let tank_ids = vec!["Tank-A1", "Tank-B2", "Tank-C3"];
    
    // Create summary results for all defined tanks
    let results: Vec<TankSummary> = tank_ids
        .into_iter()
        .map(|tank_id| {
            let mut tank_params = params.clone();
            tank_params.tank_id = Some(tank_id.to_string());
            
            // Get full analysis but only return summary
            let full_analysis = get_analysis_result(tank_params);
            
            // Convert to summary
            TankSummary {
                tank_id: full_analysis.tank_id,
                species_id: full_analysis.species_id,
                species_name: get_species_name(full_analysis.species_id),
                overall_health: full_analysis.overall_health,
                timestamp: full_analysis.timestamp,
            }
        })
        .collect();
        
    // Log timing information on completion
    let elapsed = start_time.elapsed().as_millis() as f64;
    tracing::info!(
        request_id = %request_id,
        operation = "multi_tank_analysis",
        tanks_analyzed = results.len(),
        analysis_duration_ms = elapsed,
        operation_status = "success",
        "Multi-tank environmental analysis completed"
    );
    
    Json(results)
}

// Handler for single tank analysis by ID
async fn get_tank_analysis_by_id(
    State(_state): State<AppState>,
    Path(tank_id): Path<String>,
    Query(params): Query<AnalysisParams>,
) -> impl IntoResponse {
    // Create a span for tracking single tank environmental analysis
    let span = tracing::info_span!("single_tank_analysis");
    let _guard = span.enter();
    
    // Add request ID for correlation and timing
    let request_id = uuid::Uuid::new_v4().to_string();
    let start_time = std::time::Instant::now();
    
    tracing::info!(
        request_id = %request_id,
        tank_id = %tank_id,
        operation = "single_tank_analysis",
        "Starting tank environmental analysis"
    );
    // Override tank_id from path parameter
    let mut tank_params = params;
    // Clone tank_id directly in the assignment to keep the original for logging
    tank_params.tank_id = Some(tank_id.clone());
    
    // Get single tank analysis
    let result = get_analysis_result(tank_params);
    
    // Log timing information on completion
    let elapsed = start_time.elapsed().as_millis() as f64;
    tracing::info!(
        request_id = %request_id,
        tank_id = %tank_id,
        analysis_duration_ms = elapsed,
        overall_health = %result.overall_health,
        operation_status = "success",
        "Tank environmental analysis completed"
    );
    
    Json(result)
}

// ⚠️ CHALLENGE #3: MEMORY OPTIMIZATION ⚠️
// This function creates new String objects for every analysis result
// Your task: Optimize memory usage by using static string references instead of creating new String objects

// Define static string constants for common status values
static WARNING: &str = "warning";
static CRITICAL: &str = "critical";
static NORMAL: &str = "normal";
static LOW: &str = "low";
static OVERDUE: &str = "overdue";
static EXCESS: &str = "excess";
static AT_RISK: &str = "at_risk";
static GOOD: &str = "good";
static CAUTION: &str = "caution";
static HIGH: &str = "high";
static UNKNOWN: &str = "unknown";

fn get_analysis_result(params: AnalysisParams) -> AnalysisResult {
    // Get tank_id or default to Tank-A1
    let tank_id = params.tank_id.clone().unwrap_or_else(|| "Tank-A1".to_string());
    
    // Log the analysis operation with structured fields
    tracing::debug!(
        tank_id = %tank_id,
        species_id = params.species_id,
        analysis_type = "environmental",
        operation = "tank_analysis",
        "Analyzing tank environmental conditions"
    );
    
    // Generate analysis result based on tank ID
    match tank_id.as_str() {
        "Tank-A1" => AnalysisResult {
            tank_id: tank_id,
            species_id: params.species_id.unwrap_or(1),
            timestamp: chrono::Utc::now().to_rfc3339(),
            temperature_status: WARNING,
            ph_status: CRITICAL,
            oxygen_status: NORMAL,
            feeding_status: OVERDUE,
            overall_health: AT_RISK,
            recommendations: vec![
                "Reduce temperature by 2°C",
                "Adjust pH to 7.2-7.5 range",
                "Administer emergency feeding",
            ],
        },
        "Tank-B2" => AnalysisResult {
            tank_id: tank_id,
            species_id: params.species_id.unwrap_or(2),
            timestamp: chrono::Utc::now().to_rfc3339(),
            temperature_status: NORMAL,
            ph_status: NORMAL,
            oxygen_status: LOW,
            feeding_status: NORMAL,
            overall_health: GOOD,
            recommendations: vec![
                "Increase aeration slightly",
                "Monitor oxygen levels daily",
            ],
        },
        "Tank-C3" => AnalysisResult {
            tank_id: tank_id,
            species_id: params.species_id.unwrap_or(3),
            timestamp: chrono::Utc::now().to_rfc3339(),
            temperature_status: NORMAL,
            ph_status: HIGH,
            oxygen_status: NORMAL,
            feeding_status: EXCESS,
            overall_health: CAUTION,
            recommendations: vec![
                "Reduce feeding frequency",
                "Perform 25% water change",
                "Test ammonia levels",
            ],
        },
        _ => AnalysisResult {
            tank_id: tank_id,
            species_id: params.species_id.unwrap_or(0),
            timestamp: chrono::Utc::now().to_rfc3339(),
            temperature_status: UNKNOWN,
            ph_status: UNKNOWN,
            oxygen_status: UNKNOWN,
            feeding_status: UNKNOWN,
            overall_health: UNKNOWN,
            recommendations: vec![
                "Verify tank ID",
                "Setup monitoring system",
            ],
        },
    }
}
// ⚠️ END CHALLENGE CODE ⚠️

/// Validates the implementation of Challenge #3: Memory Optimization
async fn validate_memory_optimization(
    State(_state): State<AppState>,
) -> impl IntoResponse {
    tracing::info!("Starting validation for Challenge #3: Memory Optimization");
    
    use serde_json::json;
    use std::fs;

    // Create a request ID for correlation in logs
    let request_id = uuid::Uuid::new_v4().to_string();
    
    // Extract just the challenge code section using the challenge markers
    let source_path = std::env::current_dir()
        .unwrap_or_else(|_| std::path::PathBuf::from("."))
        .join("src/main.rs");
    
    // Read the source code file
    let source_code = match fs::read_to_string(&source_path) {
        Ok(content) => content,
        Err(e) => {
            tracing::error!(
                request_id = %request_id,
                error = %e,
                "Failed to read source code for validation"
            );
            // If we can't read the source, assume the challenge is not completed
            return (StatusCode::OK, Json(json!({
                "valid": false,
                "message": "Validation failed: Unable to verify implementation.",
                "system_component": {
                    "name": "Analysis Engine ",
                    "description": "Analysis engine is experiencing high memory usage ",
                    "status": "degraded"
                }
            })));
        }
    };
    
    // Find the challenge section boundaries
    let challenge_start = source_code.find("// ⚠️ CHALLENGE #3: MEMORY OPTIMIZATION ⚠️");
    let challenge_end = source_code.find("// ⚠️ END CHALLENGE CODE ⚠️");
    
    // Check if we found the challenge section boundaries
    if challenge_start.is_none() || challenge_end.is_none() {
        tracing::error!(
            request_id = %request_id,
            "Could not find challenge section boundaries in source code"
        );
        return (StatusCode::OK, Json(json!({
            "valid": false,
            "message": "Validation failed: Unable to verify implementation.",
            "system_component": {
                "name": "Analysis Engine ",
                "description": "Analysis engine is experiencing high memory usage ",
                "status": "degraded"
            }
        })));
    }
    
    // Extract just the challenge code section
    let challenge_code = &source_code[challenge_start.unwrap()..challenge_end.unwrap() + "// ⚠️ END CHALLENGE CODE ⚠️".len()];
    
    // Check for the use of static string references
    // Count the number of .to_string() calls in the challenge code
    let to_string_count = challenge_code.matches(".to_string()").count();
    
    // Check for the use of &str instead of String
    let uses_str_type = challenge_code.contains("&str") || challenge_code.contains("&'static str");
    
    // Log what we're finding in the challenge code
    tracing::info!(
        request_id = %request_id,
        to_string_count = to_string_count,
        uses_str_type = uses_str_type,
        "Challenge code check results"
    );
    
    // The challenge is completed if the number of .to_string() calls is significantly reduced
    // and &str type is used instead
    let is_valid = to_string_count < 10 && uses_str_type;
    
    tracing::info!(
        request_id = %request_id,
        solution_valid = is_valid,
        "Challenge #3 validation completed"
    );
    
    // Build a standardized response following the same format as other challenges
    let response = json!({
        "valid": is_valid,
        "message": if is_valid {
            "Solution correctly implemented! Memory usage is now optimized."
        } else {
            "Solution validation failed. Please optimize memory usage by using static string references instead of creating new String objects."
        },
        "system_component": {
            "name": "Analysis Engine ",
            "description": if is_valid {
                "Analysis engine memory usage is now optimized"
            } else {
                "Analysis engine is experiencing high memory usage"
            },
            "status": if is_valid { "normal" } else { "degraded" }
        }
    });
    
    (StatusCode::OK, Json(response))
}
